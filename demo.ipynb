{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP+VKE/PmgQ7dCmBg+wufTg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TNAmapVAPWxo","executionInfo":{"status":"ok","timestamp":1716464497893,"user_tz":-180,"elapsed":23485,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}},"outputId":"2d67731d-b3aa-4c5b-d5cf-fc763694206e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"metadata":{"id":"BSbjoyBBPjUn","executionInfo":{"status":"ok","timestamp":1716464501833,"user_tz":-180,"elapsed":1804,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/self_drive_car/dataset/HateSpeechDataset.csv\")\n","text_df = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/self_drive_car/dataset/fake_or_real_news.csv\")"],"metadata":{"id":"QVc6wEpXPcId","executionInfo":{"status":"ok","timestamp":1716464515769,"user_tz":-180,"elapsed":10822,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["df = data[['Content', 'Label']]\n","df = df[df['Label'] != 'Label']\n","df.reset_index(drop=True, inplace=True)"],"metadata":{"id":"ZRBeJw92PlFn","executionInfo":{"status":"ok","timestamp":1716464525307,"user_tz":-180,"elapsed":429,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["text = list(text_df.text.values)\n","joined_text = ' '.join(text)\n","pratical_text = joined_text[:10000]\n","print(pratical_text[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p8EqDv_JQ4iw","executionInfo":{"status":"ok","timestamp":1716464527567,"user_tz":-180,"elapsed":404,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}},"outputId":"16b9d1aa-2b81-46d1-a90d-34ebacab6081"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Daniel Greenfield, a Shillman Journalism Fellow at the Freedom Center, is a New York writer focusing\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","stopwords = set(stopwords.words('english'))\n","print(stopwords)\n","lemmatizer = WordNetLemmatizer()\n","import string\n","def text_cleaning(a, remove_stopwords=True):\n","    # Noktalama işaretlerini kaldır\n","    remove_punc = [char for char in a if char not in string.punctuation]\n","    remove_punc = ''.join(remove_punc)\n","\n","    # Kelimeleri tokenlara ayır ve stopwords'leri çıkar (opsiyonel)\n","    tokens = word_tokenize(remove_punc)\n","    if remove_stopwords:\n","        cleaned_tokens = [word for word in tokens if word.lower() not in stopwords]\n","    else:\n","        cleaned_tokens = tokens\n","\n","    # Lemmatize et\n","    lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in cleaned_tokens]\n","\n","    return lemmatized_tokens\n","df.iloc[:,0].apply(text_cleaning)\n","from sklearn.model_selection import train_test_split\n","\n","X = df['Content']\n","y = df['Label']\n","# feature transformation - TF-IDF to the processed text feature\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# initialize the TfidfVectorizer  object\n","vectorizer = TfidfVectorizer()\n","\n","# vectorize and perform TF-IDF to the training data\n","X_vectorized = vectorizer.fit_transform(X)\n","from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB()\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","best_accuracy = 0\n","best_y_pred = None\n","best_y_test = None\n","\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Loop through each split and fit the model\n","for train_index, test_index in skf.split(X_vectorized, y):\n","    X_train, X_test = X_vectorized[train_index], X_vectorized[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    # Fit the model\n","    clf.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = clf.predict(X_test)\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(y_test, y_pred)\n","\n","    # Check if current accuracy is the best\n","    if accuracy > best_accuracy:\n","        best_accuracy = accuracy\n","        best_y_pred_temp = y_pred\n","        best_y_test_temp = y_test\n","\n","# Update best_y_pred and best_y_test with the ones associated with the best accuracy\n","best_y_pred = best_y_pred_temp\n","best_y_test = best_y_test_temp\n","\n","unique_labels_test = np.unique(best_y_test)\n","unique_labels_pred = np.unique(best_y_pred)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RmU3XyVaPqZu","executionInfo":{"status":"ok","timestamp":1716464743257,"user_tz":-180,"elapsed":204808,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}},"outputId":"b65340eb-ca90-4966-81a1-b457eec3e1d8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["{'theirs', 'your', 'in', 'above', 'some', 't', 'during', 'below', 'i', 'it', \"needn't\", 'each', 'had', 'yourself', \"you've\", 'very', 'we', 'to', 'themselves', 'who', 'can', 'most', \"wouldn't\", 'which', 'they', 'am', 'he', 'did', 'at', 'herself', 'not', 'this', 's', 'just', 'mustn', 're', 'further', 'doesn', 'his', \"she's\", 'with', 'isn', 'whom', 'all', 'under', 'a', 'and', 'does', 'been', 'of', 'only', 'she', 'being', 'shouldn', 'were', 'own', 'nor', 'yours', 'them', \"hasn't\", \"won't\", 'its', 'while', 'what', 'him', 'having', \"shan't\", 've', 'y', 'the', 'now', 'm', 'because', 'for', 'any', 'was', 'than', \"shouldn't\", \"should've\", 'be', \"wasn't\", 'o', 'ma', 'needn', 'about', 'shan', 'here', \"you're\", 'if', 'where', 'couldn', 'mightn', 'hers', \"aren't\", \"hadn't\", 'her', 'same', 'too', \"you'd\", 'should', 'once', 'wouldn', 'weren', 'are', 'between', 'few', 'other', 'my', 'but', 'do', 'into', 'itself', 'doing', 'aren', \"doesn't\", 'yourselves', 'is', 'has', 'before', 'down', 'you', 'there', 'until', 'through', 'how', 'wasn', 'ain', \"haven't\", 'on', \"mightn't\", 'off', \"couldn't\", 'such', 'an', 'out', 'hadn', 'didn', 'have', 'when', \"don't\", 'why', 'so', \"didn't\", 'hasn', 'me', 'their', 'that', 'will', 'don', \"it's\", 'our', 'after', 'd', 'haven', \"mustn't\", 'up', 'll', 'more', \"weren't\", 'as', 'ours', 'ourselves', 'won', 'myself', 'by', 'then', 'no', 'over', 'both', 'those', \"you'll\", \"that'll\", 'again', 'or', 'these', 'himself', 'from', 'against', \"isn't\"}\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import RegexpTokenizer\n","Tokenizer = RegexpTokenizer(r'\\w+')\n","tokens =  Tokenizer.tokenize(pratical_text.lower())\n","unique_tokens = np.unique(tokens)\n","unique_token_index = {token: index for index, token in enumerate(unique_tokens)}\n","n_words = 10\n","input_words = []\n","next_words = []\n","for i in range(len(tokens) - n_words):\n","    input_words.append(tokens[i:i + n_words])\n","    next_words.append(tokens[i + n_words])\n"],"metadata":{"id":"A4Ckzn37RBd1","executionInfo":{"status":"ok","timestamp":1716465811472,"user_tz":-180,"elapsed":423,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","model = load_model(\"/content/model.h5\")"],"metadata":{"id":"WAF01TdpRUt5","executionInfo":{"status":"ok","timestamp":1716465822630,"user_tz":-180,"elapsed":9046,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import random\n","def predict_next_words(input_text, n_best):\n","  input_text = input_text.lower()\n","  X = np.zeros((1, n_words, len(unique_tokens)))\n","  for i, word in enumerate(input_text.split()):\n","    X[0, i, unique_token_index[word]] = 1\n","\n","  predictions = model.predict(X)[0]\n","  return np.argpartition(predictions, -n_best)[-n_best:]\n","def generate_text(input_text, text_length, creativity=3):\n","  word_sequence = input_text.split()\n","  current=0\n","  for _ in range(text_length):\n","    sub_sequence = \" \".join(Tokenizer.tokenize( \" \".join(word_sequence).lower())[current:current+n_words])\n","    try:\n","      choice = unique_tokens[random.choice(predict_next_words(sub_sequence, creativity))]\n","    except:\n","       choice = random.choice(unique_tokens)\n","    word_sequence.append(choice)\n","    current += 1\n","  return \" \".join(word_sequence)"],"metadata":{"id":"5LsVjaT1R9HS","executionInfo":{"status":"ok","timestamp":1716465825768,"user_tz":-180,"elapsed":299,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def predict_label(sentence):\n","    cleaned_sentence = text_cleaning(sentence)\n","    print(cleaned_sentence)\n","    vectorized_sentence = vectorizer.transform(cleaned_sentence)  # Vektörleştirmede liste kullanıldı\n","    print(vectorized_sentence)\n","    prediction = clf.predict(vectorized_sentence)\n","    print(prediction)\n","\n","    # Sansürlü kelimeyi bul\n","    censored_words = [cleaned_sentence[index] for index, pred in enumerate(prediction) if pred == \"1\"]\n","    print(\"Censored words:\", censored_words)\n","\n","    # Eğer censored_words boşsa, temizlenmiş cümleyi direkt olarak döndür\n","    if not censored_words:\n","        return \" \".join(cleaned_sentence)\n","\n","    # Sansürlü kelimeleri sentence içinden sil\n","    sanitized_sentence = sentence\n","    for word in censored_words:\n","        sanitized_sentence = sanitized_sentence.replace(word, \"\")\n","\n","    return sanitized_sentence"],"metadata":{"id":"dvl-sUU7PybQ","executionInfo":{"status":"ok","timestamp":1716465828397,"user_tz":-180,"elapsed":357,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def results(sentence):\n","  answer = predict_label(sentence)\n","  answer = answer + \"insulting word\"\n","  gtext = generate_text(answer, 20,20)\n","  print(gtext)"],"metadata":{"id":"UZIotBDvSHh9","executionInfo":{"status":"ok","timestamp":1716465831130,"user_tz":-180,"elapsed":380,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["results(\"As the discussion reached its climax, the atmosphere in the hall became tense. Suddenly, one of the speakers shouted bastard\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MKdexVPMS7Cl","executionInfo":{"status":"ok","timestamp":1716457536562,"user_tz":-180,"elapsed":4,"user":{"displayName":"Betül Yaşar","userId":"04770497404425088983"}},"outputId":"59f2f16b-5a8a-447a-bc08-8762dbdf45ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['discussion', 'reached', 'climax', 'atmosphere', 'hall', 'became', 'tense', 'suddenly', 'one', 'speaker', 'shouted', 'bastard']\n","  (0, 29604)\t1.0\n","  (1, 93706)\t1.0\n","  (2, 20518)\t1.0\n","  (3, 7061)\t1.0\n","  (4, 47222)\t1.0\n","  (5, 9882)\t1.0\n","  (6, 114606)\t1.0\n","  (7, 109920)\t1.0\n","  (8, 80842)\t1.0\n","  (9, 106686)\t1.0\n","  (10, 103178)\t1.0\n","  (11, 9399)\t1.0\n","['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1']\n","Censored words: ['bastard']\n","As the discussion reached its climax, the atmosphere in the hall became tense. Suddenly, one of the speakers shouted insulting word allowed suddenly belief didn assaults taking times been proved didn re try ringing now out new hope j unite substance\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ixkvg4x5TKep"},"execution_count":null,"outputs":[]}]}